<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Testing and Continuous Integration</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				
				<section data-markdown>
					<textarea data-template>
						## Software Testing and Continuous Integration
					</textarea>
				</section>

				<section>
					<section>
						<p class="fragment">How do you know your code gives the right answers?</p>
						<br/>
						<p class="fragment">... what about after you make changes?</p>
						<br/>
						<p class="fragment">
							<strong>Legacy code:</strong> "code without tests"
							(Michael Feathers, <em>Working Effectively with Legacy Code</em>)
						</p>
					</section>

					<section>
						<h3>Testing your code</h3>
						<br/>
						<p class="fragment">when to test: <strong>always.</strong></p>
						<br/>
						<p class="fragment">where to test: <strong>external test suite.</strong></p>
						<br/>
						<p class="fragment">Example: <code>tests</code> subdirectory inside package.</p>
						<br/>
						<p class="fragment"><small>Perfect is the enemy of good; a basic level of tests is better than nothing. But a rigorous test suite will save you time and potential problems in the long run.</small></p>
					</section>

					<section>
						<h3>Why test?</h3>
						<br/>
						<p class="fragment">Testing is a core principle of scientific software; it ensures results are trustworthy.</p>
						<br/>
						<p class="fragment">Scientific and engineering software is used for planes, power plants, satellites, and decisionmaking.
							Thus, correctness of this software is pretty important.
						</p>
						<br/>
						<p class="fragment"><small>And we all know how easy it is to have mistakes in code without realizing it...</small>
						</p>
					</section>

					<section>
						<p class="fragment">
							Testing is particularly important if you want contributions to your codebase.
						</p>
						<ul>
							<li class="fragment">
								As a contributor, how do I know if my PR hasn’t broken anything?
							</li>
							<li class="fragment">
								As a commiter, how do I know if your PR is ok?
							</li>
							<li class="fragment">
								Unit tests are often a great repository of how different features of a library actually work
							</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h3>What and how to test?</h3>
						<br/>

						<pre><code data-trim class="python fragment">
							def kepler_loc(p1, p2, dt, t):
								'''Use Kepler's Laws to predict location of 
								celestial body'''
								...
								return p3
						</code></pre>

						<br/>
						<pre><code data-trim class="python fragment">
							def test_kepler_loc():
								p1 = jupiter(two_days_ago)
								p2 = jupiter(yesterday)
								exp = jupiter(today)
								obs = kepler_loc(p1, p2, 1, 1)
								if exp != obs:
									raise ValueError(
										"Jupiter is not where it should be!"
										)
						</code></pre>
					</section>

					<section>
						<h3>What is a test?</h3>
						<br/>

						<blockquote>
							Tests compare expected outputs versus observed outputs for known inputs.
							They do not inspect the body of the function directly.
							In fact, the body of a function does not even have to exist for a valid
							test to be written.
						</blockquote>

						<pre><code data-trim class="python fragment">
							def test_func():
								exp = get_expected()
								obs = func(*args, **kwargs)
								assert exp == obs
						</code></pre>
					</section>

					<section>
						<h3>Good idea: test through assertions</h3>

						<p class="fragment">For exactness:</p>
						<pre><code data-trim class="python fragment">
							def test_kepler_loc():
								p1 = jupiter(two_days_ago)
								p2 = jupiter(yesterday)
								exp = jupiter(today)
								obs = kepler_loc(p1, p2, 1, 1) 
								assert exp == obs
						</code></pre>
						<p class="fragment">For approximate exactness:</p>
						<pre><code data-trim class="python fragment">
							import numpy as np
							def test_kepler_loc():
								p1 = jupiter(two_days_ago)
								p2 = jupiter(yesterday)
								exp = jupiter(today)
								obs = kepler_loc(p1, p2, 1, 1) 
								assert np.allclose(exp, obs)
						</code></pre>
					</section>

					<section>
						<h3>Test using <a href="https://docs.pytest.org/en/latest/"><code>pytest</code></a></h3>
						<br/>

						<pre><code data-trim class="python fragment">
							# content of test_sample.py
							def inc(x):
								return x + 1

							def test_answer():
								assert inc(3) == 5
						</code></pre>

						<pre><code data-trim class="bash fragment">
							$ pytest
						</code></pre>

						<p class="fragment"><code>pytest</code> finds all testing modules and functions, and runs them.</p>
					</section>
				</section>
				<section>
					<section>
						<h3>Kinds of tests</h3><br/>

						<p class="fragment"><strong>interior test</strong>: precise points/values do not matter</p>

						<p class="fragment"><strong>edge test</strong>: test examines beginning or end of a range</p>

						<p class="fragment">Best practice: test all edges and at least one interior point.</p>
					</section>

					<section>
						<p class="fragment">Also <strong>corner cases</strong>: two or more edge cases combined.</p>
						<br/>
						<pre><code data-trim class="python fragment">
							import numpy as np

							def sinc2d(x, y):
								'''(Describe the function here)'''
								if x == 0.0 and y == 0.0:
									return 1.0
								elif x == 0.0:
									return np.sin(y) / y
								elif y == 0.0:
									return np.sin(x) / x
								else:
									return (np.sin(x) / x) * (np.sin(y) / y)
						</code></pre>
					</section>

					<section>
						<pre><code data-trim class="python">
							import numpy as np
							from mod import sinc2d

							def def test_internal():
								exp = (2.0 / np.pi) * (-2.0 / (3.0 * np.pi))
								obs = sinc2d(np.pi / 2.0, 3.0 * np.pi / 2.0)
								assert np.allclose(exp, obs)
							def test_edge_x():
								exp = (-2.0 / (3.0 * np.pi))
								obs = sinc2d(0.0, 3.0 * np.pi / 2.0)
								assert np.allclose(exp, obs)
						</code></pre>
						<pre><code data-trim class="python">
							def test_edge_y():
								exp = (2.0 / np.pi)
								obs = sinc2d(np.pi / 2.0, 0.0)
								assert np.allclose(exp, obs)
							def test_corner():
								exp = 1.0
								obs = sinc2d(0.0, 0.0)
								assert np.allclose(exp, obs)
						</code></pre>
					</section>

					<section>
						<h3>Test generators</h3>
	
						<pre class="stretch"><code data-trim class="python">
							import numpy as np
							import pytest
	
							# contents of add.py
							def add2(x, y):
								return x + y
	
							@pytest.mark.parametrize('exp, x, y', [
								(4, 2, 2),
								(5, -5, 10),
								(42, 40, 2),
								(16, 3, 13),
								(-128, 0, -128),
							])
							def test_add2(self, x, y, exp):
								obs = add2(x, y)
								assert np.allclose(exp, obs)
						</code></pre>
					</section>

					<section>
						<h3>Types of tests</h3><br/>
	
						<ul>
							<li class="fragment"><strong>unit test</strong>: interrogate individual functions and methods
							</li>
							<li class="fragment"><strong>integration test</strong>: verify that multiple pieces of the code work together
							</li>
							<li class="fragment"><strong>regression test</strong>: confirm that results match prior code results (which are assumed correct)
							</li>
						</ul>
					</section>

					<section>
						<p>Developers warn against having unit tests without any integration tests</p>
						<img width="40%" data-src="./images/unit-tests-dryer.gif">
						<img width="40%" data-src="./images/unit-tests-doors.gif">
						
						<small>
							Sources: <a href="https://twitter.com/thepracticaldev/status/845638950517706752">twitter/@ThePracticalDev</a>,
							<a href="https://twitter.com/withzombies/status/829716565834752000">twitter/@withzombies</a>
						</small>
					</section>
				</section>

				<section>
					<section>
						<h3>Test-Driven Development (TDD)</h3><br/>

						<p class="fragment">Write the tests first.</p>

						<p class="fragment">Before you write any lines of a function, first write the test for that function.</p>
					</section>

					<section>
						<pre><code data-trim class="python">
							from numpy import allclose
							from mod import std

							def test_std1():
								obs = std([0.0, 2.0])
								exp = 1.0
								assert np.allclose(exp, obs)
						</code></pre>

						<pre><code data-trim class="python fragment">
							def std(vals):
								# this must be cheating.
								return 1.0
						</code></pre>
					</section>

					<section>
						<pre><code data-trim class="python">
							def test_std1():
								obs = std([0.0, 2.0])
								exp = 1.0
								assert np.allclose(exp, obs)

							def test_std2():
								obs = std()
								exp = 0.0
								assert np.allclose(exp, obs)

							def test_std3():
								obs = std([0.0, 4.0])
								exp = 2.0
								assert np.allclose(exp, obs)
						</code></pre>

						<pre><code data-trim class="python fragment">
							def std(vals):
								# a bit better, but still not quite generic
								if len(vals) == 0:
									return 0.0
								return vals[-1] / 2.0
						</code></pre>
					</section>

					<section>
						<div style="font-size:25px">
						<pre class="stretch"><code data-trim class="python">
							def test_std1():
								obs = std([0.0, 2.0])
								exp = 1.0
								assert np.allclose(exp, obs)

							def test_std2():
								obs = std()
								exp = 0.0
								assert np.allclose(exp, obs)

							def test_std3():
								obs = std([0.0, 4.0])
								exp = 2.0
								assert np.allclose(exp, obs)

							def test_std4():
								obs = std([1.0, 3.0])
								exp = 1.0
								assert np.allclose(exp, obs)

							def test_std4():
								obs = std([1.0, 1.0, 1.0])
								exp = 0.0
								assert np.allclose(exp, obs)
						</code></pre>
						<pre><code data-trim class="python fragment">
							def std(vals):
								# finally some math
								n = len(vals)
								if n == 0:
									reutnr 0.0
								mu = sum(vals) / n
								var = 0.0
								for val in vals:
									var = var + (val - mu)**2
								return (var / n)**0.5
						</code></pre>
						</div>
					</section>
				</section>

				<section>
					<section>
						<h2>Other testing tips</h2>
						<br/>
						<p class="fragment">Consider <a href="https://pep8.org">PEP8</a> 
							testing to ensure consistency in your codebase, 
							regardless of the number of people contributing.
						</p>
						<pre><code data-trim class="bash fragment">
							pip install flake8
							flake8 .
						</code></pre>
					</section>

					<section>
						<p>Test raising exceptions</p>

						<pre><code data-trim class="python">
							import pytest
							def test_zero_division():
    							with pytest.raises(ZeroDivisionError):
        							1 / 0
						</code></pre>
					</section>

					<section>
						<h3>Mocking in unit tests</h3>
						
						<ul>
							<li class="fragment">
								Mocks allow you replace parts of your system with precisely controllable code
								<ul>
									<li>Specify return values</li>
									<li>Throw exceptions</li>
								</ul>
							</li>
							<li class="fragment">
								Assertions to Verify mock interactions
								<ul>
									<li>Was the method called?</li>
									<li>Was it called with the expected arguments?</li>
								</ul>
							</li>
							<li class="fragment">
								A good unit test should only exercise the code in your module and mock out all dependencies
							</li>
						</ul>
					</section>
					<section>
						<p>Example with mocking:</p>
						<div style="font-size:35px">
						<pre class="stretch"><code data-trim class="python">
							def test_create_update_path(self, mocker):
								import elasticsearch
								mock_es = mocker.MagicMock(elasticsearch.Elasticsearch)
								mocker.patch.object(
									elasticsearch, 'Elasticsearch', return_value=mock_es
									)
								adaptor = ElasticSearchAdapter('localhost', '9999', 'foo', 'bar')

								adaptor.create_update_path("123-456", {
									'name': 'my-nane',
									'description': 'this is a test'
								})

								mock_es.index.assert_called_with(
									body={
										'name': 'my-name',
										'description': 'this is a test'
									},
									index='servicex_paths',
									id='123-456',
								)
						</code></pre>
						</div>
					</section>

				</section>

				<section>
					<section>
						<h2>Test coverage</h2>
						<br/>
						<p class="fragment">Meaning: percentage of code for which a test exists,
							determined by number of line executed during tests
						</p>
					</section>

					<section>
						<h2><a href="https://pytest-cov.readthedocs.io"><code>pytest-cov</code></a></h2>
						<br/>
						<p class="fragment">Instructions:</p>
						<ol>
							<li class="fragment">
								Install <code>pytest-cov</code> using <code>pip</code>/<code>conda</code>
							</li>
							<li class="fragment">
								<code>pytest -vv --cov=./</code>
							</li>
							<li class="fragment">
								Look at coverage; are you at or near 100%?
							</li>
							<li class="fragment">
								Get more detailed information by having it create a report:
								<code>pytest -vv --cov=./ --cov-report html</code>
							</li>
						</ol>
					</section>

					<section>
						<h2>Example</h2>
						<br/>
						<pre><code data-trim class="fragment python">
							# content of test_sample.py
							def inc(x):
								if x < 0:
									return x - 1
								return x + 1

							def test_answer():
								assert inc(3) == 4
						</code></pre>

						<pre><code data-trim class="fragment bash">
							$ pytest -vv test_sample.py --cov=./

							main.py::test_answer PASSED

							---------- coverage: platform darwin, python 3.5.4-final-0 -----------
							Name      Stmts   Miss  Cover
							-----------------------------
							main.py       6      1    83%
						</code></pre>
					</section>

					<section>
						<h2>Coverage report</h2>
						<br/>
						<img src="./images/coverage-report.png" height="400" alt="Example HTML coverage report from pytest-cov">
					</section>

					<section>
						<pre><code data-trim class="fragment python">
							# content of test_sample.py
							def inc(x):
								if x < 0:
									return x - 1
								return x + 1

							def test_answer():
								assert inc(3) == 4

							def test_answer_negative():
								assert inc(-2) == -3
						</code></pre>

						<pre><code data-trim class="fragment bash">
							$ pytest -vv test_sample.py --cov=./

							main.py::test_answer PASSED
							main.py::test_answer_negative PASSED

							---------- coverage: platform darwin, python 3.5.4-final-0 -----------
							Name      Stmts   Miss  Cover
							-----------------------------
							main.py       8      0   100%
						</code></pre>
					</section>

					<section>
						<h3>Coverage overview: work towards 100%</h3>
						<br/>
						<h3 class="fragment">Use coverage to help you identify missing edge/corner cases</h3>
					</section>
				</section>

				<section>
                    <section>
                        <h2>Next: continuous integration</h2>
                        <br/>
                        <p class="fragment">Meaning: ensure all changes to your project pass tests through automated
                            test & build process
                        </p>
					</section>
					
					<section>
						<h2>Importance of testing + continuous integration</h2>

						<img class="plain" width="60%" src="./images/chemical-shifts-paper.png"/>

						<small>
							<a href="https://doi.org/10.1021/acs.orglett.9b03216"><code>https://doi.org/10.1021/acs.orglett.9b03216</code></a>
						</small>
					</section>

					<section>
						<p class="fragment">
							Recent paper found differences in calculated nuclear magnetic resonance chemical shifts on different operating systems
						</p>
						<p class="fragment">
							Using Python scripts published in Nature Protocols in 2014, cited over 130 times
						</p>
						<p class="fragment">
							Error due to differences in file sorting across platforms… 
						</p>
					</section>

                    <section>
                        <h3>Getting started with <a href="https://travis-ci.org">Travis CI</a>:</h3>
                        <br/>
                        <ol>
                            <li class="fragment">
                                Sign in at <a href="https://travis-ci.org">travis-ci.org</a> with your GitHub account
                            </li>
                            <li class="fragment">
                                Go to your profile and enable the repository for the software you want to build & test.
                            </li>
                            <li class="fragment">
                                Add a <code>.travis.yml</code> file to the repository
                            </li>
                            <li class="fragment">
                                Add info to configuration file telling Travis CI what to do:
                                what programming language (<code>language: python</code>), what it needs to build your
                                package (<code>install</code>), and how to run the tests (<code>language: script</code>)
                            </li>
                        </ol>
                    </section>

                    <section>
                        <h3>Using pip</h3>
                        <br/>
                        <pre><code data-trim class="python">
                            language: python
                            python:
                            - "3.5"
                            - "3.6"

                            install:
                                - pip install -r requirements.txt
                            script:
                                - pytest -vv --cov=./;
                        </code></pre>
                    </section>

                    <section>
                        <h4>Contents of <code>requirements.txt</code></h4>
                        <pre><code data-trim>
                            #
                            ####### requirements.txt #######
                            #
                            ###### Requirements without Version Specifiers ######
                            pytest
                            pytest-cov
                            #
                            ###### Requirements with Version Specifiers ######
                            #   See https://www.python.org/dev/peps/pep-0440/#version-specifiers
                            numpy >= 1.12.0
                            pint >= 0.7.2
                            #
                        </code></pre>
                    </section>

                    <section>
                        <h3>Using conda</h3>
                        <br/>
                        <div style="font-size:30px">
                        <pre class="stretch"><code data-trim class="bash">
                            language: python
                            python:
                            - "3.5"
                            - "3.6"

                            install:
                                - if [[ "$TRAVIS_OS_NAME" == "linux" ]]; then
                                    wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;
                                  elif [[ "$TRAVIS_OS_NAME" == "osx" ]]; then
                                    wget https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -O miniconda.sh;
                                  fi
                                - bash miniconda.sh -b -p $HOME/miniconda
                                - rm miniconda.sh
                                - source $HOME/miniconda/etc/profile.d/conda.sh && conda activate
                                - conda config --set always_yes yes --set changeps1 no
                                - conda update -q conda
                                - conda update -q --all
                                - conda config --append channels conda-forge
                                - conda env create -qf test-environment.yaml;
                                - source activate test-environment;
                            script:
                                - pytest -vv --cov=./;
                        </code></pre>
                        </div>
                    </section>

                    <section>
                        <h4>Contents of <code>test-environment.yaml</code></h4>
                        <pre><code data-trim class="yaml">
                            name: test-environment
                            channels:
                            - defaults
                            - conda-forge
                            dependencies:
                              - python=3.5
                              - numpy>=1.12.0
                              - pytest>=3.2.0
                              - pytest-cov
                              - pint>=0.7.2
                        </code></pre>
                    </section>

                    <section>
                        <h3>Getting started with <a href="https://travis-ci.org">Travis CI</a> (contd.)</h3>
                        <br/>
                        <ol start="5">
                            <li class="fragment">
                                <code>git add .travis.yml test-environment.yaml</code>, commit, and push to GitHub
                            </li>
                            <li class="fragment">
                                Check the <a href="https://travis-ci.org/auth">build status page</a> to see if the build passes or fails
                            </li>
                            <li class="fragment">
                                (Optional) Add a status badge to your README:
                                <a href="https://travis-ci.org/travis-ci/travis-web"><img src="https://travis-ci.org/travis-ci/travis-web.svg?branch=master" alt="Build Status"></a>
                                (Get this from your Repository page on Travis CI)
                            </li>
                        </ol>
                        <br/>
                        <p class="fragment">GitHub will use Travis CI to tell you if Pull Requests are safe to merge!</p>
                    </section>

                </section>

			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
